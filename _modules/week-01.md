---
title: 第一周
---

Jan 9
: **讲座**{: .label .label-blue }[Introduction (介绍)](#)
  : Percy Liang

Jan 11
: **主题**{: .label .label-green }[FlashAttention](#)   
  : [Tri Dao](https://tridao.me) (来自斯坦福大学)
: ***阅读材料***
<!-- 有关Transformer和GPU的两篇短博客文章（可选，但推荐阅读）。 -->
<!-- - **Transformers** -->
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (可选，但推荐阅读!)  
<!-- - **Deep learning on GPUs**   -->
- [Making Deep Learning Go Brrrr From First Principles.](https://horace.io/brrr_intro.html)
- [NVIDIA deep learning performance guide.](https://docs.nvidia.com/deeplearning/performance/index.html) (可选)

#### 阅读材料
- 有关Transformer和GPU的两篇短博客文章：
  - [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - 介绍Transformer的图解。
  - [Attention is All You Need](https://arxiv.org/abs/1706.03762) - 论文，可选，但推荐阅读。

- 有关深度学习和GPU的阅读材料：
  - [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html) - 从第一原理出发让深度学习加速的文章。
  - [NVIDIA deep learning performance guide](https://docs.nvidia.com/deeplearning/performance/index.html) - NVIDIA深度学习性能指南。

#### 说明
- 第一周课程提供了一个关于Transformer模型和GPU加速深度学习的基本介绍和资源列表。
- 特别提到了一篇重要的论文《Attention Is All You Need》，这篇论文详细介绍了Transformer模型的架构和原理，对于理解现代深度学习模型中的注意力机制至关重要。
- 还提供了一些辅助材料，帮助读者更好地理解深度学习在GPU上的性能优化。